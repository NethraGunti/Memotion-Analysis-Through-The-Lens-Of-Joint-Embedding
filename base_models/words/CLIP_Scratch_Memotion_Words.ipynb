{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLIP_Scratch_Memotion_Words.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6C4LiHWFfun"
      },
      "source": [
        "!rm -Rf sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mHirwW_YdJe",
        "outputId": "274a750f-951a-409e-fadf-cabf2383923d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvQbTly4Ffu2"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OTxhS1gFfu2"
      },
      "source": [
        "!cp /content/drive/MyDrive/Wipro/Data/memotion1_images_combined.zip /content/drive/MyDrive/Wipro/Data/image_word_pairs.csv ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nHTXMJqFfu2"
      },
      "source": [
        "!unzip -q memotion1_images_combined"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1nKCNIEFfu2"
      },
      "source": [
        "!rm memotion1_images_combined.zip merged_data.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuxXkdIMFfu3",
        "outputId": "87047686-4e8e-485d-c891-2aa435d7b396"
      },
      "source": [
        "import os\n",
        "len(os.listdir('images'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8870"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDJwtYwuFfu3"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GsYQCgHFfu3",
        "outputId": "12079383-fb3d-4ae4-a615-9ec33dae0d19"
      },
      "source": [
        "!pip install timm\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.4.12)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.10.0+cu102)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.2)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0siT17iVFfu4"
      },
      "source": [
        "import shutil\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import transformers\n",
        "from transformers import CLIPModel, CLIPConfig\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os \n",
        "import pandas as pd\n",
        "import cv2 as cv\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "from sklearn.metrics import  f1_score\n",
        "from tqdm.autonotebook import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorboard as tb\n",
        "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
        "\n",
        "\n",
        "from tensorboard.plugins import projector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBHS8R4XFfu4"
      },
      "source": [
        "# CLIPModel(CLIPConfig())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDPFuDJTFfu4"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPTYpkh2Ffu4",
        "outputId": "7266f55b-09a9-4abe-f6e0-81a4bc8ea324"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxJTMFVyFfu6",
        "outputId": "a1477a33-9376-4e83-9c9e-a0ef61567c8d"
      },
      "source": [
        "# CLIP Tokeniser\n",
        "tokeniser = transformers.CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmX1Q-m7Ffu6"
      },
      "source": [
        "class AvgMeter:\n",
        "    def __init__(self, name=\"Metric\"):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.avg, self.sum, self.count = [0] * 3\n",
        "\n",
        "    def update(self, val, count=1):\n",
        "        self.count += count\n",
        "        self.sum += val * count\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __repr__(self):\n",
        "        text = f\"{self.name}: {self.avg:.4f}\"\n",
        "        return text\n",
        "\n",
        "def get_lr(optimizer):\n",
        "    for param_group in optimizer.param_groups:\n",
        "        return param_group[\"lr\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kL9EZwJFfu7"
      },
      "source": [
        "'''\n",
        "  Dataset class\n",
        "'''\n",
        "\n",
        "class MemotionData(Dataset):\n",
        "    \n",
        "\n",
        "    def __init__(self,root_dir,img_names,ocr_text,sentiments,tokeniser,transforms=None):\n",
        "        \n",
        "        self.img_names = img_names\n",
        "        self.ocr_text = ocr_text\n",
        "        self.sentiments = sentiments\n",
        "        self.tokeniser = tokeniser\n",
        "        self.root_dir = root_dir\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.img_names)\n",
        "        \n",
        "        \n",
        "    def __getitem__(self,idx):\n",
        "        \n",
        "        # print(self.img_names[idx])\n",
        "\n",
        "        img = cv.imread(os.path.join(self.root_dir,self.img_names[idx]))\n",
        "        img = cv.cvtColor(img,cv.COLOR_BGR2RGB)\n",
        "        \n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "        \n",
        "        output_token_ids = self.tokeniser.encode_plus(\n",
        "            self.ocr_text[idx],\n",
        "            max_length=76,\n",
        "            add_special_tokens=True,\n",
        "            padding='max_length',\n",
        "            return_tensors='pt',\n",
        "            return_attention_mask=True,\n",
        "            truncation = True\n",
        "        )\n",
        "        \n",
        "        return {\n",
        "            'img': torch.FloatTensor(img),\n",
        "            'input_ids': output_token_ids['input_ids'],\n",
        "            'att_mask': output_token_ids['attention_mask'],\n",
        "            'sentiment': torch.tensor(self.sentiments[idx],dtype=torch.long)\n",
        "        }\n",
        "        \n",
        "\n",
        "\n",
        "def create_dataset(df,tokeniser,max_len=76):\n",
        "    ds = MemotionData(\n",
        "        root_dir = './images/',\n",
        "        img_names = df['image_name'].to_list(),\n",
        "        ocr_text= df['word'].to_list(),\n",
        "        sentiments = df['overall_sentiment'].to_list(),\n",
        "        tokeniser = tokeniser,\n",
        "        transforms = torchvision.transforms.Compose(\n",
        "        [\n",
        "                torchvision.transforms.ToPILImage(),\n",
        "                torchvision.transforms.Resize((224,224)),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "        ]\n",
        "        )\n",
        "    )\n",
        "    return ds        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apk3t35kFfu7"
      },
      "source": [
        "'''\n",
        "  To create model based on CLIP \n",
        "'''\n",
        "class MemotionModel(nn.Module):\n",
        "    def __init__(self,scratch=True):\n",
        "        super(MemotionModel,self).__init__()\n",
        "        self.pre_model = CLIPModel(CLIPConfig()).to(device)\n",
        "        self.scratch = scratch\n",
        "        \n",
        "        if scratch:\n",
        "          for params in self.pre_model.parameters():\n",
        "              params.requires_grad = True\n",
        "        \n",
        "        else:\n",
        "          for params in self.pre_model.parameters():\n",
        "              params.requires_grad = False\n",
        "                \n",
        "    def forward(self,x,input_ids,att_mask):\n",
        "        img_embed =  self.pre_model.get_image_features(x)\n",
        "        text_embed = self.pre_model.get_text_features(input_ids.squeeze(1),attention_mask=att_mask.squeeze(1))\n",
        "        return img_embed, text_embed\n",
        "        \n",
        "\n",
        "def calc_loss(image_embeddings, text_embeddings, temperature=1.0):\n",
        "    logits = (text_embeddings @ image_embeddings.T) / temperature\n",
        "    images_similarity = image_embeddings @ image_embeddings.T\n",
        "    texts_similarity = text_embeddings @ text_embeddings.T\n",
        "    targets = F.softmax(\n",
        "        (images_similarity + texts_similarity) / 2 * temperature, dim=-1\n",
        "    )\n",
        "    texts_loss = cross_entropy(logits, targets, reduction='none')\n",
        "    images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
        "    loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
        "    return logits, targets, loss.mean()\n",
        "\n",
        "def cross_entropy(preds, targets, reduction='none'):\n",
        "    log_softmax = nn.LogSoftmax(dim=-1)\n",
        "    loss = (-targets * log_softmax(preds)).sum(1)\n",
        "    if reduction == \"none\":\n",
        "        return loss\n",
        "    elif reduction == \"mean\":\n",
        "        return loss.mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nS0z6YhLFfu7",
        "outputId": "77f68314-6607-409b-c189-90869dc60f4f"
      },
      "source": [
        "df = pd.read_csv('image_word_pairs.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_name</th>\n",
              "      <th>word</th>\n",
              "      <th>overall_sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>image_4737.png</td>\n",
              "      <td>films</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>image_6131.png</td>\n",
              "      <td>2:11</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>image_4709.jpg</td>\n",
              "      <td>spiderman</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>image_2842.png</td>\n",
              "      <td>morpheus</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>image_6734.jpg</td>\n",
              "      <td>picard</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       image_name       word overall_sentiment\n",
              "0  image_4737.png      films          positive\n",
              "1  image_6131.png       2:11          positive\n",
              "2  image_4709.jpg  spiderman          positive\n",
              "3  image_2842.png   morpheus          positive\n",
              "4  image_6734.jpg     picard          positive"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYWwvhQ7Ffu8",
        "outputId": "11a44f18-a6fa-4d6d-f326-685f0cbd57ed"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58267, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNrSJEUBFfu8",
        "outputId": "4e17f8c5-2e89-4a8b-f896-b1a3d4c743ce"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['image_name', 'word', 'overall_sentiment'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toCpInvDFfu8"
      },
      "source": [
        "'''\n",
        "  Defining Labels\n",
        "'''\n",
        "\n",
        "def cvt_sentiment(senti):\n",
        "    if senti in ['very_positive','positive']:\n",
        "        return 2\n",
        "    elif senti in ['very_negative','negative']:\n",
        "        return 0\n",
        "    else: \n",
        "        return 1\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzNJ_auQFfu8"
      },
      "source": [
        "'''\n",
        "  Remove unwanted images\n",
        "'''\n",
        "\n",
        "df_new = df.copy()\n",
        "df_new['overall_sentiment'] = df_new['overall_sentiment'].apply(cvt_sentiment)\n",
        "rem_images = ['image_1567.jpg','image_4924.jpg','image_5119.png','image_6357.jpg']\n",
        "df_new.drop(df_new[df_new['image_name'].isin(rem_images)].index,inplace=True)\n",
        "df_new.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eYgZxUf1Ffu9",
        "outputId": "ede43490-d042-4660-b3b3-99fe8e9889e1"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(58267, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pM1iC1xFfu9",
        "outputId": "837fa696-6373-44c3-9786-4a88617a6576"
      },
      "source": [
        "(df_new['overall_sentiment'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    34552\n",
              "1    18472\n",
              "0     5191\n",
              "Name: overall_sentiment, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk7i-sF7Ffu9"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-iY7Z95Ffu-"
      },
      "source": [
        "'''\n",
        "  Train, Val ( 80-10 )\n",
        "'''\n",
        "\n",
        "df_train,df_val = train_test_split(df_new,test_size=0.2)\n",
        "# df_val,df_test = train_test_split(df_test,test_size=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta_sqCy5Ffu-",
        "outputId": "cd90fb0f-dcec-421a-b0f6-d154c1fe2a73"
      },
      "source": [
        "df_train.shape,df_val.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((46572, 3), (11643, 3))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY6ypvnZFfu-"
      },
      "source": [
        "# Create dataset\n",
        "\n",
        "train_dataset = create_dataset(df_train,tokeniser)\n",
        "val_dataset = create_dataset(df_val,tokeniser)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVgZ4QilFfu_"
      },
      "source": [
        "'''\n",
        "  Return Weights for training\n",
        "'''\n",
        "\n",
        "def ret_sample_weights(df_new,train_dataset):\n",
        "\n",
        "  class_counts = list(np.unique(df_new['overall_sentiment'],return_counts=True)[1])\n",
        "  class_weights = [sum(class_counts)/c for c in class_counts]\n",
        "\n",
        "  sample_weights = [0]*len(train_dataset)\n",
        "\n",
        "  for idx,x in enumerate(train_dataset):\n",
        "    class_weight = class_weights[x['sentiment']]\n",
        "    sample_weights[idx] = class_weight\n",
        "\n",
        "  return sample_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fwT0iv7Ffu_"
      },
      "source": [
        "sample_weights = ret_sample_weights(df_new,train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfufK7SKFfu_"
      },
      "source": [
        "len(sample_weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSPzqmxtFfu_"
      },
      "source": [
        "'''\n",
        "  Loaders for training \n",
        "'''\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_sampler = WeightedRandomSampler(sample_weights,num_samples=len(sample_weights),replacement=True)\n",
        "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,sampler=train_sampler)\n",
        "# test_loader = DataLoader(test_dataset,batch_size=BATCH_SIZE)\n",
        "val_loader = DataLoader(val_dataset,batch_size=BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-9hYxk3FfvA"
      },
      "source": [
        "for i in train_loader:\n",
        "    print(i.keys())\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRmPAxScFfvA"
      },
      "source": [
        "'''\n",
        "  To check is labels are balanced\n",
        "'''\n",
        "\n",
        "temp = [0,0,0]\n",
        "for x in train_loader:\n",
        "  # print(x['sentiment'].shape)\n",
        "  # break\n",
        "  for y in x['sentiment']:\n",
        "    # print(y['sentiment'])\n",
        "    temp[int(y)]+=1\n",
        "temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUlv58OvFfvA"
      },
      "source": [
        "def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
        "    loss_meter = AvgMeter()\n",
        "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
        "    for batch in tqdm_object:\n",
        "\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        image_embeddings, text_embeddings = model(batch['img'], batch['input_ids'], batch['att_mask'])\n",
        "        logits, targets, loss = calc_loss(image_embeddings, text_embeddings)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if step == \"batch\":\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        count = batch[\"img\"].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "\n",
        "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
        "    return loss_meter\n",
        "\n",
        "\n",
        "def valid_epoch(model, valid_loader):\n",
        "    loss_meter = AvgMeter()\n",
        "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
        "    for batch in tqdm_object:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        image_embeddings, text_embeddings = model(batch['img'], batch['input_ids'], batch['att_mask'])\n",
        "        logits, targets, loss = calc_loss(image_embeddings, text_embeddings)\n",
        "\n",
        "        count = batch[\"img\"].size(0)\n",
        "        loss_meter.update(loss.item(), count)\n",
        "\n",
        "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
        "    \n",
        "    return loss_meter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1x2gxRg3FfvB"
      },
      "source": [
        "head_lr = 1e-3\n",
        "image_encoder_lr = 1e-4\n",
        "text_encoder_lr = 1e-5\n",
        "weight_decay = 1e-3\n",
        "patience = 1\n",
        "factor = 0.8\n",
        "epochs = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dV6ZbqrcFfvB"
      },
      "source": [
        "import itertools\n",
        "\n",
        "model = MemotionModel().to(device)\n",
        "params = [\n",
        "    {\"params\": model.parameters(), \"lr\": image_encoder_lr},\n",
        "]\n",
        "optimizer = torch.optim.AdamW(params, weight_decay=weight_decay)\n",
        "lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode=\"min\", patience=patience, factor=factor\n",
        ")\n",
        "step = \"epoch\"\n",
        "\n",
        "best_loss = float('inf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmikw0YvFfvB"
      },
      "source": [
        "training_loss = []\n",
        "validation_loss = []\n",
        "for epoch in range(epochs):\n",
        "    if (epoch!=0) and ((epoch+1)%5==0):\n",
        "        torch.save(model.state_dict(),'./clip_scratch_memotion_words_basic_model.pt')\n",
        "    print(f\"Epoch: {epoch + 1}\")\n",
        "    model.train()\n",
        "\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n",
        "    print(f'train_loss: {train_loss}')\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        valid_loss = valid_epoch(model, val_loader)\n",
        "    print(f'valid_loss: {valid_loss}')\n",
        "    if valid_loss.avg < best_loss:\n",
        "        best_loss = valid_loss.avg\n",
        "        torch.save(model.state_dict(), \"best.pt\")\n",
        "        print(\"Saved Best Model!\")\n",
        "    \n",
        "    lr_scheduler.step(valid_loss.avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXHDtshKqSdF"
      },
      "source": [
        "!mv best.pt clip_scratch_memotion_words_basic_model.pt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVL4JHejFfvB"
      },
      "source": [
        "!cp clip_scratch_memotion_words_basic_model.pt /content/drive/MyDrive/Wipro/Implementation/models/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sbH5peeTiPa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}